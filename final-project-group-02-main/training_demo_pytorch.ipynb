{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pEdwBK3noX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Custom label smoothing loss with class weights\n",
        "class LabelSmoothingCrossEntropyWeighted(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        logprobs = nn.functional.log_softmax(x, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        if self.class_weights is not None:\n",
        "            weights = self.class_weights[target]\n",
        "            loss = weights * (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
        "        else:\n",
        "            loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n",
        "# Neural network model\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(input_size, hidden_size), nn.SiLU(), nn.Dropout(dropout_rate)]\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(hidden_size, hidden_size), nn.SiLU(), nn.Dropout(dropout_rate)]\n",
        "        layers.append(nn.Linear(hidden_size, output_size))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Load and preprocess data\n",
        "X = np.loadtxt(\"Xtr.csv\", delimiter=\",\")\n",
        "y = np.loadtxt(\"ytr.csv\", delimiter=\",\")\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=seed\n",
        ")\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Model configuration\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "num_layers = 5\n",
        "output_size = len(np.unique(y))\n",
        "dropout_rate = 0.05\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 20000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data and model to device\n",
        "X_train, X_val = X_train.to(device), X_val.to(device)\n",
        "y_train, y_val = y_train.to(device), y_val.to(device)\n",
        "model = NeuralNet(input_size, hidden_size, num_layers, output_size, dropout_rate).to(device)\n",
        "criterion = LabelSmoothingCrossEntropyWeighted(smoothing=0.1, class_weights=class_weights_tensor.to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Train model\n",
        "best_val_bacc = 0\n",
        "best_epoch = 0\n",
        "model_save_path = \"best_model_dropout_005_l2.pth\"\n",
        "\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    model.train()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = model(X_val)\n",
        "        val_preds = torch.argmax(val_logits, dim=1)\n",
        "        val_bacc = balanced_accuracy_score(y_val.cpu().numpy(), val_preds.cpu().numpy())\n",
        "\n",
        "        if val_bacc > best_val_bacc:\n",
        "            best_val_bacc = val_bacc\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | Val BACC: {val_bacc:.4f} | Best: {best_val_bacc:.4f} @ {best_epoch}\")\n",
        "            correct = (val_preds == y_val).cpu().numpy()\n",
        "            for cls in np.unique(y):\n",
        "                cls_mask = y_val.cpu().numpy() == cls\n",
        "                cls_acc = correct[cls_mask].sum() / cls_mask.sum()\n",
        "                print(f\"Class {cls}: {cls_acc:.4f}\", end=\" | \")\n",
        "            print()\n",
        "\n",
        "print(f\"Best Val Balanced Accuracy: {best_val_bacc:.4f} at epoch {best_epoch}\")\n",
        "\n",
        "# Save traced model on CPU\n",
        "model.to(\"cpu\")\n",
        "x = torch.randn(1, input_size)  # shape: (1, 9)\n",
        "with torch.no_grad():\n",
        "    traced_model = torch.jit.trace(model, x)\n",
        "torch.jit.save(traced_model, \"model_neuralnet.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load TorchScript model (must be traced on CPU)\n",
        "model = torch.jit.load(\"model_neuralnet.pth\")\n",
        "model.eval()\n",
        "\n",
        "# Load data\n",
        "Xka = np.loadtxt(\"Xka.csv\", delimiter=\",\")\n",
        "Xtr = np.loadtxt(\"Xtr.csv\", delimiter=\",\")\n",
        "\n",
        "# Standardize using Xtr\n",
        "scaler = StandardScaler()\n",
        "Xtr_std = scaler.fit_transform(Xtr)\n",
        "Xka_std = scaler.transform(Xka)\n",
        "\n",
        "# Predict on CPU\n",
        "Xka_tensor = torch.tensor(Xka_std, dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    yka_hat = model(Xka_tensor).argmax(dim=1).numpy()\n",
        "\n",
        "# Save prediction\n",
        "np.savetxt(\"yka_hat_neuralnet.csv\", yka_hat, fmt=\"%d\", delimiter=\",\")\n"
      ],
      "metadata": {
        "id": "7ML8Aty83pvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}